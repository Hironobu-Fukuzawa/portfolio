{"cells":[{"cell_type":"markdown","source":["## Task 1: Spark SQL (15m)"],"metadata":{"id":"yvjBmGBAxnQc","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36b565b0-dc53-4172-b646-4c82e1c472be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"MkbrHZYEw5Cr","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30d54257-dc20-4174-aa40-84e1f6abc56f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sales_file_location = \"/FileStore/tables/Sales_table.csv\"\nproducts_file_location = \"/FileStore/tables/Products_table.csv\"\nsellers_file_location = \"/FileStore/tables/Sellers_table.csv\"\nfile_type = \"csv\"\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf_pt = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(products_file_location)\n\ndf_sales = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sales_file_location)\n\ndf_seller = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sellers_file_location)\n"],"metadata":{"id":"2luSAeOXxBiQ","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5607f10e-0a58-4330-bbeb-fa1d6863efb1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# (a) Output the top 3 most popular products sold among all sellers [2m]\n\n# df_pt.createOrReplaceTempView(\"PT\")\n# df_sales.createOrReplaceTempView(\"SALES\")\n# sql_text =\"\"\"\n# SELECT PT.product_name\n# FROM PT INNER JOIN SALES ON PT.product_id = SALES.product_id\n# GROUP BY PT.product_name\n# ORDER BY SUM(SALES.num_of_items_sold) DESC\n# LIMIT 3;\n# \"\"\"\n# result = spark.sql(sql_text)\n\nfrom pyspark.sql.functions import expr, desc\n\nresult = df_pt.join(df_sales, \"product_id\")\\\n.groupBy(\"product_name\")\\\n.agg(expr(\"sum(num_of_items_sold)\").alias(\"total_number_of_items\"))\\\n.orderBy(desc(\"total_number_of_items\"))\\\n.limit(3)\\\n.select(\"product_name\")\n\nresult.show()\n\n# +-------------+\n# | product_name|\n# +-------------+\n# |product_51270|\n# |product_18759|\n# |product_59652|\n# +-------------+\n"],"metadata":{"id":"Ps_v7oTixnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7fb33021-930c-4fa9-b595-4ed83c279ed4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n| product_name|\n+-------------+\n|product_51270|\n|product_18759|\n|product_59652|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (b) Output the top most sold product (in terms of quantity) among sellers with seller_id 1 to 10 [2m]\n# Your table should have 1 column(s): [product_name] \n\nfrom pyspark.sql.functions import expr, col, desc\n\nresult = df_pt.join(df_sales, \"product_id\")\\\n.groupBy(\"seller_id\", \"product_name\")\\\n.agg(expr(\"sum(num_of_items_sold)\").alias(\"total_quantity\"))\\\n.orderBy(desc(\"total_quantity\"))\\\n.filter((col(\"seller_id\") <= 10) & (col(\"seller_id\") >= 1))\\\n.limit(1)\\\n.select(\"product_name\")\n\nresult.show()\n\n# +-------------+\n# | product_name|\n# +-------------+\n# |product_36658|\n# +-------------+\n"],"metadata":{"id":"Ljmb_1OaxC8Q","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"866983b3-8214-4740-8f4d-90e87d1db482","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n| product_name|\n+-------------+\n|product_36658|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (c) Compute the combined revenue earned from sellers where seller_id ranges from 1 to 500 inclusive. [3m]\n# Your table should have 1 column(s): [total_revenue]\nfrom pyspark.sql.functions import expr, col, sum as _sum\n\nresult = df_pt.join(df_sales, \"product_id\")\\\n.groupBy(\"seller_id\")\\\n.agg(expr(\"sum(num_of_items_sold * price)\").alias(\"revenue\"))\\\n.orderBy(desc(\"revenue\"))\\\n.filter((col(\"seller_id\") <= 500) & (col(\"seller_id\") >= 1))\\\n.select(_sum(\"revenue\").alias(\"total_revenue\"))\n\nresult.show()\n\n# +-------------+\n# |total_revenue|\n# +-------------+\n# |    160916699|\n# +-------------+\n"],"metadata":{"id":"QtinRRycxDBS","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa7bec8e-f93d-48ff-af38-d395c6fe7422","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|total_revenue|\n+-------------+\n|    160916699|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (d) Among sellers with rating >= 4 who have achieved a combined number of products sold >= 3000, find out the top 10 most expensive product sold by any of the sellers. (If there are multiple products at the same price, please sort them in ascending order of product_id) [8m]\n# Your table should have 1 column(s): [product_name]\n# To get the full mark, your query should not run for more than 1 min\n\nfrom pyspark.sql.functions import expr, desc, asc, col, sum as _sum\n\ndf_max_num_by_seller = df_sales.join(\n    df_seller.filter(col(\"rating\") >= 4),\n    on=\"seller_id\",\n    how=\"inner\"\n).groupBy(\"seller_id\")\\\n.agg(expr(\"sum(num_of_items_sold)\").alias(\"total_num_of_items_per_seller\"))\\\n.orderBy(desc(\"total_num_of_items_per_seller\"))\\\n.filter(col(\"total_num_of_items_per_seller\") >= 3000)\\\n.select(\"*\")\n\ndf_result = df_max_num_by_seller.join(df_sales, on=[\"seller_id\"], how=\"left\") \\\n    .join(df_pt, on=[\"product_id\"], how=\"left\")\\\n    .select(\"product_name\", \"price\")\\\n    .distinct()\\\n    .orderBy(desc(\"price\"), asc(\"product_id\"))\\\n    .limit(10)\\\n    .select(\"product_name\", \"price\")\n\ndf_result.show()\n\n# +------------+-----+\n# |product_name|price|\n# +------------+-----+\n# | product_106|  200|\n# | product_117|  200|\n# | product_363|  200|\n# | product_712|  200|\n# | product_843|  200|\n# | product_897|  200|\n# | product_923|  200|\n# |product_1466|  200|\n# |product_1507|  200|\n# |product_1514|  200|\n# +------------+-----+\n"],"metadata":{"id":"jdG80LVMxnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59c00e0a-34de-4614-b783-71beb7503716","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+-----+\n|product_name|price|\n+------------+-----+\n| product_106|  200|\n| product_117|  200|\n| product_363|  200|\n| product_712|  200|\n| product_843|  200|\n| product_897|  200|\n| product_923|  200|\n|product_1466|  200|\n|product_1507|  200|\n|product_1514|  200|\n+------------+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Task 2: Spark ML (10m)"],"metadata":{"id":"4fziMyvTxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2551ab92-377c-4492-9d99-258610b143a1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"wtocOKQXxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ebc093d-9256-4e99-85d3-3d36b50a6053","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bank_train_location = \"/FileStore/tables/bank_train.csv\"\nbank_test_location = \"/FileStore/tables/bank_test.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nbank_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_train_location)\n\nbank_test = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_test_location)\n\nbank_train.show()\nbank_test.show()"],"metadata":{"id":"lQB18KhnxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2eee140e-773a-4e76-9f6c-40e809e136b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+--------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-----+\n|age|       job| marital|education|default|balance|housing|loan|  contact|day|month|duration|campaign|pdays|previous|poutcome|label|\n+---+----------+--------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-----+\n| 45|    admin.| married|  unknown|     no|   2033|     no|  no| cellular| 28|  may|      48|       4|   -1|       0| unknown|    0|\n| 56|    admin.| married|  primary|     no|    202|    yes|  no|  unknown|  9|  may|     178|       2|   -1|       0| unknown|    0|\n| 50| housemaid|  single|secondary|     no|    799|     no|  no|telephone| 28|  jan|      63|       1|   -1|       0| unknown|    0|\n| 58|    admin.| married|secondary|     no|   1464|    yes| yes|  unknown|  5|  jun|      53|      29|   -1|       0| unknown|    0|\n| 43|management|  single| tertiary|     no|  11891|     no|  no| cellular|  4|  dec|     821|       5|  242|       1| success|    1|\n| 61|   retired| married|secondary|     no|    938|     no|  no| cellular| 15|  jul|     392|       2|  183|       3| success|    1|\n| 40|technician|divorced|secondary|     no|    275|     no|  no| cellular| 13|  aug|     409|       4|   -1|       0| unknown|    0|\n| 52|  services| married|secondary|     no|    961|     no| yes| cellular| 18|  feb|     222|       1|  553|       4| failure|    1|\n| 36|    admin.| married|secondary|     no|    953|    yes|  no| cellular| 17|  feb|      38|       1|   -1|       0| unknown|    0|\n| 60|unemployed| married|secondary|     no|   1047|     no|  no| cellular| 27|  may|     173|       3|  279|       4| success|    1|\n| 37| housemaid| married|secondary|     no|    187|     no|  no|  unknown| 12|  jun|     165|       1|   -1|       0| unknown|    0|\n| 33|management|  single| tertiary|     no|    191|     no| yes| cellular|  3|  feb|     678|       4|  209|       1| failure|    0|\n| 31|management| married| tertiary|     no|    216|     no|  no| cellular| 22|  jul|      47|       1|   -1|       0| unknown|    0|\n| 54|technician| married|secondary|     no|      0|     no|  no| cellular|  6|  nov|     464|       5|  192|       8|   other|    1|\n| 47|management| married| tertiary|     no|    600|     no|  no| cellular| 19|  aug|     290|      15|   -1|       0| unknown|    1|\n| 21|   student|  single|secondary|     no|     71|     no|  no| cellular| 13|  jan|     169|       2|   -1|       0| unknown|    1|\n| 43|management| married| tertiary|     no|      8|     no|  no| cellular| 11|  aug|      95|       4|   -1|       0| unknown|    0|\n| 63|   retired| married|secondary|     no|  18016|     no|  no| cellular|  7|  jul|     371|       1|   -1|       0| unknown|    1|\n| 48|    admin.| married|  unknown|     no|      0|    yes|  no| cellular|  8|  may|      85|       1|  168|       2| failure|    0|\n| 53|    admin.| married|secondary|     no|   1796|     no|  no|telephone| 16|  mar|     295|       1|   91|       2| success|    1|\n+---+----------+--------+---------+-------+-------+-------+----+---------+---+-----+--------+--------+-----+--------+--------+-----+\nonly showing top 20 rows\n\n+---+------------+--------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-----+\n|age|         job| marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|label|\n+---+------------+--------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-----+\n| 45|  management| married| tertiary|     no|   2220|    yes|  no|cellular| 11|  jul|     128|       2|   -1|       0| unknown|    0|\n| 36| blue-collar|  single|secondary|     no|   3623|     no|  no| unknown| 12|  nov|      71|       1|  378|       1| success|    1|\n| 37|  management| married|  primary|     no|   1506|     no|  no|cellular|  2|  nov|     101|       3|   80|       3| success|    0|\n| 65|      admin.| married|secondary|     no|    952|     no|  no|cellular|  6|  sep|     255|       1|   96|       1| success|    1|\n| 37|  management| married| tertiary|     no|     40|     no|  no|cellular| 27|  aug|    1033|       4|   -1|       0| unknown|    1|\n| 45|entrepreneur|divorced| tertiary|     no|   -395|    yes|  no| unknown| 13|  may|     470|       1|   -1|       0| unknown|    1|\n| 51|  management| married|secondary|     no|   1573|     no|  no|cellular| 28|  aug|      72|      18|   -1|       0| unknown|    0|\n| 39| blue-collar|  single|  primary|     no|      0|    yes| yes|cellular| 13|  may|    1975|       5|   -1|       0| unknown|    1|\n| 34|  management| married| tertiary|     no|    606|     no|  no|cellular| 20|  apr|      97|       1|   -1|       0| unknown|    1|\n| 30|  management|  single| tertiary|     no|    596|     no|  no|cellular|  4|  jun|     125|       2|   -1|       0| unknown|    0|\n| 60|   housemaid| married|  primary|     no|   1163|     no|  no|cellular| 30|  apr|     470|       8|   -1|       0| unknown|    1|\n| 45|  management|divorced|secondary|     no|     51|    yes|  no|cellular| 31|  jul|      67|       8|   -1|       0| unknown|    0|\n| 30|    services|  single|secondary|     no|    140|    yes|  no|cellular| 15|  may|     760|       1|   -1|       0| unknown|    1|\n| 38|  technician|  single| tertiary|     no|   2496|     no|  no| unknown| 17|  jun|     193|       1|   -1|       0| unknown|    0|\n| 37|  technician| married| tertiary|    yes|      0|    yes|  no| unknown| 14|  may|      93|      14|   -1|       0| unknown|    0|\n| 41| blue-collar| married|  primary|     no|   6596|    yes|  no|cellular| 20|  nov|      88|       1|   -1|       0| unknown|    0|\n| 45|entrepreneur| married|  unknown|     no|   3133|    yes| yes|cellular| 10|  jul|     804|       1|   -1|       0| unknown|    1|\n| 33|      admin.|  single| tertiary|     no|    193|     no|  no| unknown|  5|  may|     132|       2|   -1|       0| unknown|    0|\n| 31|  technician|  single| tertiary|     no|    985|    yes|  no|cellular| 17|  apr|     997|       1|   57|       2| failure|    1|\n| 49|  management| married| tertiary|     no|    271|    yes|  no|cellular|  5|  feb|      18|       6|   -1|       0| unknown|    0|\n+---+------------+--------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Build ML model to predict whether the customer will subscribe bank deposit service or not. Train the model using training set and evaluate the model performance (e.g. accuracy) using testing set. \n* You can explore different methods to pre-process the data and select proper features\n* You can utilize different machine learning models and tune model hyperparameters\n* Present the final testing accuracy."],"metadata":{"id":"YTZevHlAxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98477bc0-fdf9-4585-8cf2-24b4b0ebc3f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# data preparation (4m)\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.feature import ChiSqSelector\n\n# Convert categorical variables into Integer values\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']]\n\n# OneHotEncoder\nencoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_vec\") for column in ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']]\n\n# Combine all features\nassembler = VectorAssembler(inputCols=['age', 'job_vec', 'marital_vec', 'education_vec', 'default_vec', 'balance', 'housing_vec', 'loan_vec', 'contact_vec', 'day', 'month_vec', 'duration', 'campaign', 'pdays', 'previous', 'poutcome_vec'], outputCol=\"all_features\")\n\n# Select proper number of features\nselector = ChiSqSelector(numTopFeatures=45, featuresCol=\"all_features\", outputCol=\"selected_features\", labelCol=\"label\")\n"],"metadata":{"id":"iey06VQfxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e07aaf5a-6fb8-425a-a3c9-f52e04e49828","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# model building (4m)\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nrf = RandomForestClassifier(numTrees=10, featureSubsetStrategy=\"auto\", impurity=\"gini\", maxDepth=4, maxBins=32, seed=42, featuresCol=\"selected_features\", labelCol=\"label\")\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, selector, rf])\n\n# Grid Search Targets (numTopFeatures)\n# 1st time: Optimal number of top features: 45 (30-50)\nparamGrid = ParamGridBuilder() \\\n    .addGrid(rf.numTrees, [10, 15, 20]) \\\n    .addGrid(rf.maxDepth, [4, 6, 8]) \\\n    .addGrid(selector.numTopFeatures, [42, 44, 46, 48, 50]) \\\n    .build()\n\naccuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n\n# Cross Validation\ncrossval = CrossValidator(\n    estimator=pipeline,\n    estimatorParamMaps=paramGrid,\n    evaluator=accuracy_evaluator,\n    numFolds=3,\n    seed=42\n)\n\ncv_model = crossval.fit(bank_train)\n\n# Print most appropriate number of features\noptimal_num_top_features = cv_model.bestModel.stages[-2].getNumTopFeatures()\nprint(\"Optimal number of top features: {}\".format(optimal_num_top_features))\n"],"metadata":{"id":"PsIotb9ExnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a04b59e4-6197-451c-8071-52526a5a724f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Optimal number of top features: 42\n"]}],"execution_count":0},{"cell_type":"code","source":["# model evaluation (2m)\npredictions = cv_model.transform(bank_test)\n\naccuracy = accuracy_evaluator.evaluate(predictions)\n\nprint(accuracy)\n"],"metadata":{"id":"OC5ufJqAxnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80e1c949-8291-45be-8872-c0310777c6fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["0.8356471115091805\n"]}],"execution_count":0}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"application/vnd.databricks.v1+notebook":{"notebookName":"cs5245_a2_databricks_ A0255967J","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2659203281378118}},"nbformat":4,"nbformat_minor":0}
